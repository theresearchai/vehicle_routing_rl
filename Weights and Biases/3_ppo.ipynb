{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:pytorch4]",
      "language": "python",
      "name": "conda-env-pytorch4-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "3.ppo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miMDooS4kg26",
        "outputId": "bcf6dcb4-a28f-41c9-baf5-c55b4475cb4c"
      },
      "source": [
        "!pip install import wandb\n",
        "import math\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: import in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.6/dist-packages (0.10.12)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.19.5)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.1.12)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.0.1)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: watchdog>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.0.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (51.1.1)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tAMwe0Gkg3B"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLATuuI-kg3D"
      },
      "source": [
        "<h2>Use CUDA</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ytK4_kNkg3F"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVqmJgG6kg3H"
      },
      "source": [
        "<h2>Create Environments</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjFfQmz4krln",
        "outputId": "104686f9-c891-4fcc-e91c-f727e6b01ce7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWhgvdS5kg3I"
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/MyDrive/RL-Adventure-2/common')\n",
        "\n",
        "from common.multiprocessing_env import SubprocVecEnv\n",
        "import common\n",
        "num_envs = 16\n",
        "env_name = \"Pendulum-v0\"\n",
        "\n",
        "def make_env():\n",
        "    def _thunk():\n",
        "        env = gym.make(env_name)\n",
        "        return env\n",
        "\n",
        "    return _thunk\n",
        "\n",
        "envs = [make_env() for i in range(num_envs)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "\n",
        "env = gym.make(env_name)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCRwLEAukg3K"
      },
      "source": [
        "<h2>Neural Network</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYL6V-TWkg3L"
      },
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
        "        nn.init.constant_(m.bias, 0.1)\n",
        "        \n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        \n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "        \n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(num_inputs, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, num_outputs),\n",
        "        )\n",
        "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
        "        \n",
        "        self.apply(init_weights)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        value = self.critic(x)\n",
        "        mu    = self.actor(x)\n",
        "        std   = self.log_std.exp().expand_as(mu)\n",
        "        dist  = Normal(mu, std)\n",
        "        return dist, value"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG1mfCqykg3N"
      },
      "source": [
        "def plot(frame_idx, rewards):\n",
        "    clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
        "    plt.plot(rewards)\n",
        "    plt.show()\n",
        "    \n",
        "def test_env(vis=False):\n",
        "    state = env.reset()\n",
        "    if vis: env.render()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
        "        state = next_state\n",
        "        if vis: env.render()\n",
        "        total_reward += reward\n",
        "    return total_reward"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye9oNzr7kg3P"
      },
      "source": [
        "<h2>GAE</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GZZFtW6kg3Q"
      },
      "source": [
        "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * tau * masks[step] * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rPs2_0-kg3R"
      },
      "source": [
        "<h1> Proximal Policy Optimization Algorithm</h1>\n",
        "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msHl4ne3kg3S"
      },
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
        "        \n",
        "        \n",
        "\n",
        "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "            dist, value = model(state)\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            ratio = (new_log_probs - old_log_probs).exp()\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
        "\n",
        "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    #print (state,entropy,loss)\n",
        "    wandb.log({\n",
        "        \"state\": state,\n",
        "        \"entropy\": entropy,\n",
        "        \"loss\": loss})\n",
        "\n",
        "            "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "M0ONChShkg3V",
        "outputId": "74dbbfc5-92fc-4166-bb56-df9c0f28b173"
      },
      "source": [
        "num_inputs  = envs.observation_space.shape[0]\n",
        "num_outputs = envs.action_space.shape[0]\n",
        "wandb.init(project=\"pytorch-mnist\")\n",
        "config = wandb.config   \n",
        "#Hyper params:\n",
        "config.hidden_size      = 256\n",
        "config.lr               = 3e-4\n",
        "num_steps        = 20\n",
        "config.mini_batch_size  = 5\n",
        "ppo_epochs       = 4\n",
        "config.threshold_reward = -200\n",
        "\n",
        "model = ActorCritic(num_inputs, num_outputs, config.hidden_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "wandb.watch(model, log=\"all\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrlinvrp\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.12<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">legendary-bush-17</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/rlinvrp/pytorch-mnist\" target=\"_blank\">https://wandb.ai/rlinvrp/pytorch-mnist</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/rlinvrp/pytorch-mnist/runs/pbxov8iv\" target=\"_blank\">https://wandb.ai/rlinvrp/pytorch-mnist/runs/pbxov8iv</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210111_185226-pbxov8iv</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<wandb.wandb_torch.TorchGraph at 0x7f90cb48e630>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdUyUmNSkg3X"
      },
      "source": [
        "max_frames = 15000\n",
        "frame_idx  = 0\n",
        "test_rewards = []"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "M630m1owkg3g",
        "outputId": "991ef0e2-19b7-4798-ad06-dd19ba1dc4bd"
      },
      "source": [
        "state = envs.reset()\n",
        "early_stop = False\n",
        "\n",
        "while frame_idx < max_frames and not early_stop:\n",
        "\n",
        "    log_probs = []\n",
        "    values    = []\n",
        "    states    = []\n",
        "    actions   = []\n",
        "    rewards   = []\n",
        "    masks     = []\n",
        "    entropy = 0\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        dist, value = model(state)\n",
        "\n",
        "        action = dist.sample()\n",
        "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "\n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy += dist.entropy().mean()\n",
        "        \n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
        "        \n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        \n",
        "        state = next_state\n",
        "        frame_idx += 1\n",
        "        \n",
        "        if frame_idx % 1000 == 0:\n",
        "            test_reward = np.mean([test_env() for _ in range(10)])\n",
        "            test_rewards.append(test_reward)\n",
        "            plot(frame_idx, test_rewards)\n",
        "            if test_reward > config.threshold_reward: early_stop = True\n",
        "            \n",
        "\n",
        "    next_state = torch.FloatTensor(next_state).to(device)\n",
        "    _, next_value = model(next_state)\n",
        "    returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "    returns   = torch.cat(returns).detach()\n",
        "    log_probs = torch.cat(log_probs).detach()\n",
        "    values    = torch.cat(values).detach()\n",
        "    states    = torch.cat(states)\n",
        "    actions   = torch.cat(actions)\n",
        "    advantage = returns - values   \n",
        "    ppo_update(ppo_epochs, config.mini_batch_size, states, actions, log_probs, returns, advantage)\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34cSWvxLMKYI",
        "outputId": "67dfbbfb-3f8d-47f9-9bbc-a9056f559384"
      },
      "source": [
        "wandb.save('model.h5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1t6uSg7kg3i"
      },
      "source": [
        "<h1>Saving trajectories for GAIL</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpITzWntkg3k",
        "outputId": "95a936e3-1259-4992-8000-8fd8843458a5"
      },
      "source": [
        "from itertools import count\n",
        "\n",
        "max_expert_num = 50000\n",
        "num_steps = 0\n",
        "expert_traj = []\n",
        "\n",
        "for i_episode in count():\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    \n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        action = dist.sample().cpu().numpy()[0]\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        expert_traj.append(np.hstack([state, action]))\n",
        "        num_steps += 1\n",
        "    \n",
        "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
        "    \n",
        "    if num_steps >= max_expert_num:\n",
        "        break\n",
        "        \n",
        "expert_traj = np.stack(expert_traj)\n",
        "print()\n",
        "print(expert_traj.shape)\n",
        "print()\n",
        "np.save(\"expert_traj.npy\", expert_traj)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0 reward: -133.5056485070341\n",
            "episode: 1 reward: -3.3737309166625002\n",
            "episode: 2 reward: -135.0328820133956\n",
            "episode: 3 reward: -131.27964142064513\n",
            "episode: 4 reward: -125.12845453838382\n",
            "episode: 5 reward: -4.247933460422459\n",
            "episode: 6 reward: -395.59297834503883\n",
            "episode: 7 reward: -253.25736991568547\n",
            "episode: 8 reward: -135.50603026103278\n",
            "episode: 9 reward: -132.72095459732952\n",
            "episode: 10 reward: -133.89608385869212\n",
            "episode: 11 reward: -4.5990508813314035\n",
            "episode: 12 reward: -134.44470210766775\n",
            "episode: 13 reward: -801.7661346371387\n",
            "episode: 14 reward: -131.97725229377644\n",
            "episode: 15 reward: -266.76940521674015\n",
            "episode: 16 reward: -247.5062278004002\n",
            "episode: 17 reward: -4.914595620774103\n",
            "episode: 18 reward: -138.7990887577753\n",
            "episode: 19 reward: -268.3754189751262\n",
            "episode: 20 reward: -363.28764882256417\n",
            "episode: 21 reward: -128.15870842354997\n",
            "episode: 22 reward: -134.94598918501788\n",
            "episode: 23 reward: -309.9577786212293\n",
            "episode: 24 reward: -131.91670030817002\n",
            "episode: 25 reward: -134.65823444568952\n",
            "episode: 26 reward: -134.5615349098279\n",
            "episode: 27 reward: -273.5740578550409\n",
            "episode: 28 reward: -265.05553942459926\n",
            "episode: 29 reward: -258.0591054576666\n",
            "episode: 30 reward: -128.91060595426686\n",
            "episode: 31 reward: -656.2461074160591\n",
            "episode: 32 reward: -136.84071690580248\n",
            "episode: 33 reward: -259.2365200533221\n",
            "episode: 34 reward: -132.68644155022494\n",
            "episode: 35 reward: -260.66364797902054\n",
            "episode: 36 reward: -128.8211009270027\n",
            "episode: 37 reward: -384.53615237759317\n",
            "episode: 38 reward: -4.612904346743044\n",
            "episode: 39 reward: -401.1162060114804\n",
            "episode: 40 reward: -126.25334578262932\n",
            "episode: 41 reward: -3.845934927726255\n",
            "episode: 42 reward: -132.44253012402612\n",
            "episode: 43 reward: -134.1267203432647\n",
            "episode: 44 reward: -128.56866661753938\n",
            "episode: 45 reward: -4.97856955649956\n",
            "episode: 46 reward: -392.498679426522\n",
            "episode: 47 reward: -4.756869243844947\n",
            "episode: 48 reward: -4.59189846851519\n",
            "episode: 49 reward: -4.7496626929539225\n",
            "episode: 50 reward: -131.08999767991665\n",
            "episode: 51 reward: -138.17235302513578\n",
            "episode: 52 reward: -3.751761058079555\n",
            "episode: 53 reward: -260.6317126814632\n",
            "episode: 54 reward: -4.535299319594524\n",
            "episode: 55 reward: -133.70892423024802\n",
            "episode: 56 reward: -134.8732103854694\n",
            "episode: 57 reward: -5.315182694344295\n",
            "episode: 58 reward: -265.04898120165\n",
            "episode: 59 reward: -124.99288470795233\n",
            "episode: 60 reward: -4.247632479535832\n",
            "episode: 61 reward: -3.68334723705883\n",
            "episode: 62 reward: -133.617727327027\n",
            "episode: 63 reward: -136.28353948776376\n",
            "episode: 64 reward: -5.056124136459314\n",
            "episode: 65 reward: -262.7844771770983\n",
            "episode: 66 reward: -251.52420165781922\n",
            "episode: 67 reward: -133.4014820950796\n",
            "episode: 68 reward: -7.0558924646711\n",
            "episode: 69 reward: -135.41150554590206\n",
            "episode: 70 reward: -131.8871841825757\n",
            "episode: 71 reward: -130.8724972571845\n",
            "episode: 72 reward: -367.7339135957503\n",
            "episode: 73 reward: -134.25198778254116\n",
            "episode: 74 reward: -133.86858295338342\n",
            "episode: 75 reward: -378.9443227440811\n",
            "episode: 76 reward: -3.5473336732949625\n",
            "episode: 77 reward: -261.5470895641183\n",
            "episode: 78 reward: -408.34135925288217\n",
            "episode: 79 reward: -257.6727990499033\n",
            "episode: 80 reward: -399.78682205537433\n",
            "episode: 81 reward: -266.08087229456055\n",
            "episode: 82 reward: -817.186490578741\n",
            "episode: 83 reward: -4.500140134501902\n",
            "episode: 84 reward: -508.65456581456573\n",
            "episode: 85 reward: -378.46002005145874\n",
            "episode: 86 reward: -137.76181809972095\n",
            "episode: 87 reward: -674.8280917415572\n",
            "episode: 88 reward: -128.65034230393303\n",
            "episode: 89 reward: -3.922315525193146\n",
            "episode: 90 reward: -131.00005239353024\n",
            "episode: 91 reward: -130.68974732718007\n",
            "episode: 92 reward: -135.21946982972375\n",
            "episode: 93 reward: -137.3667851983452\n",
            "episode: 94 reward: -136.9119001250973\n",
            "episode: 95 reward: -254.5371556381929\n",
            "episode: 96 reward: -374.827391591992\n",
            "episode: 97 reward: -523.9964989484117\n",
            "episode: 98 reward: -133.94200200894622\n",
            "episode: 99 reward: -133.74880434577523\n",
            "episode: 100 reward: -247.32247835568552\n",
            "episode: 101 reward: -138.75528548988993\n",
            "episode: 102 reward: -4.847096453940289\n",
            "episode: 103 reward: -136.62732481247133\n",
            "episode: 104 reward: -262.20300946977864\n",
            "episode: 105 reward: -6.5435854338994\n",
            "episode: 106 reward: -125.17361036750681\n",
            "episode: 107 reward: -690.5202921080676\n",
            "episode: 108 reward: -280.53617631459497\n",
            "episode: 109 reward: -135.40352441695322\n",
            "episode: 110 reward: -131.07617970631023\n",
            "episode: 111 reward: -247.0260554601557\n",
            "episode: 112 reward: -135.40673404514774\n",
            "episode: 113 reward: -395.03306256658476\n",
            "episode: 114 reward: -384.1784417792837\n",
            "episode: 115 reward: -128.4500742980931\n",
            "episode: 116 reward: -463.6977661877445\n",
            "episode: 117 reward: -130.94801971085445\n",
            "episode: 118 reward: -144.0228791279258\n",
            "episode: 119 reward: -667.2634492717342\n",
            "episode: 120 reward: -131.79948959004724\n",
            "episode: 121 reward: -138.03140142705894\n",
            "episode: 122 reward: -129.26779443720966\n",
            "episode: 123 reward: -3.2877798185337896\n",
            "episode: 124 reward: -134.72016283865193\n",
            "episode: 125 reward: -382.2159098741087\n",
            "episode: 126 reward: -264.6491917411121\n",
            "episode: 127 reward: -134.2254720027939\n",
            "episode: 128 reward: -424.8235005744391\n",
            "episode: 129 reward: -134.52619102883028\n",
            "episode: 130 reward: -537.7406839640856\n",
            "episode: 131 reward: -133.90654715605245\n",
            "episode: 132 reward: -132.20198118805123\n",
            "episode: 133 reward: -400.3589991495165\n",
            "episode: 134 reward: -130.12695949420717\n",
            "episode: 135 reward: -290.86810229081595\n",
            "episode: 136 reward: -394.9043391522139\n",
            "episode: 137 reward: -133.42125091255778\n",
            "episode: 138 reward: -134.96306459417266\n",
            "episode: 139 reward: -3.8499366797706336\n",
            "episode: 140 reward: -3.828788719469504\n",
            "episode: 141 reward: -5.554963437941836\n",
            "episode: 142 reward: -4.510403163975261\n",
            "episode: 143 reward: -325.97799775791754\n",
            "episode: 144 reward: -3.1174779530363375\n",
            "episode: 145 reward: -134.55262416681552\n",
            "episode: 146 reward: -350.45777263184095\n",
            "episode: 147 reward: -137.33235583532627\n",
            "episode: 148 reward: -452.0061280718382\n",
            "episode: 149 reward: -265.98673902850385\n",
            "episode: 150 reward: -284.8590382363739\n",
            "episode: 151 reward: -250.06981206461143\n",
            "episode: 152 reward: -129.50428228187013\n",
            "episode: 153 reward: -393.09302439930724\n",
            "episode: 154 reward: -5.075964808667517\n",
            "episode: 155 reward: -129.83816358490287\n",
            "episode: 156 reward: -266.1020126434327\n",
            "episode: 157 reward: -132.23463644630868\n",
            "episode: 158 reward: -779.5855091317233\n",
            "episode: 159 reward: -3.763971510946643\n",
            "episode: 160 reward: -132.67794144748086\n",
            "episode: 161 reward: -662.5587064643477\n",
            "episode: 162 reward: -135.2401324340408\n",
            "episode: 163 reward: -259.9633585943629\n",
            "episode: 164 reward: -6.232862086437321\n",
            "episode: 165 reward: -139.498411973157\n",
            "episode: 166 reward: -135.35070491390638\n",
            "episode: 167 reward: -135.1400077480551\n",
            "episode: 168 reward: -347.3664683729514\n",
            "episode: 169 reward: -427.1984854733556\n",
            "episode: 170 reward: -5.15672209428849\n",
            "episode: 171 reward: -525.916662268042\n",
            "episode: 172 reward: -133.7053511504196\n",
            "episode: 173 reward: -271.26784680564384\n",
            "episode: 174 reward: -124.85474506625023\n",
            "episode: 175 reward: -134.19873581079943\n",
            "episode: 176 reward: -255.83160338962983\n",
            "episode: 177 reward: -135.13400569542506\n",
            "episode: 178 reward: -4.960226836538054\n",
            "episode: 179 reward: -139.19809065222032\n",
            "episode: 180 reward: -140.05080094044732\n",
            "episode: 181 reward: -137.76647105767526\n",
            "episode: 182 reward: -403.1731636539886\n",
            "episode: 183 reward: -257.970427512537\n",
            "episode: 184 reward: -3.7473226459331066\n",
            "episode: 185 reward: -278.3098063643893\n",
            "episode: 186 reward: -255.99692458401518\n",
            "episode: 187 reward: -4.6365121508813445\n",
            "episode: 188 reward: -244.67627722290948\n",
            "episode: 189 reward: -131.21920785362062\n",
            "episode: 190 reward: -777.3698354491825\n",
            "episode: 191 reward: -132.07220706141683\n",
            "episode: 192 reward: -392.09434598281683\n",
            "episode: 193 reward: -136.06354238422503\n",
            "episode: 194 reward: -377.4409927865957\n",
            "episode: 195 reward: -132.18253486880235\n",
            "episode: 196 reward: -129.15162595976702\n",
            "episode: 197 reward: -396.5254064840202\n",
            "episode: 198 reward: -3.610361833207753\n",
            "episode: 199 reward: -245.53736015092704\n",
            "episode: 200 reward: -270.99181854480565\n",
            "episode: 201 reward: -247.4231450110685\n",
            "episode: 202 reward: -131.59894474370887\n",
            "episode: 203 reward: -144.7898370619998\n",
            "episode: 204 reward: -926.5588068852352\n",
            "episode: 205 reward: -133.39727923189105\n",
            "episode: 206 reward: -131.93566436017008\n",
            "episode: 207 reward: -6.40529176710689\n",
            "episode: 208 reward: -257.08448208556194\n",
            "episode: 209 reward: -130.92098423630432\n",
            "episode: 210 reward: -262.2927047192545\n",
            "episode: 211 reward: -6.859901180492491\n",
            "episode: 212 reward: -262.70877767928914\n",
            "episode: 213 reward: -134.56588203218894\n",
            "episode: 214 reward: -135.22465193371625\n",
            "episode: 215 reward: -137.9657247788344\n",
            "episode: 216 reward: -135.13425433384725\n",
            "episode: 217 reward: -132.3215993693809\n",
            "episode: 218 reward: -400.611961792729\n",
            "episode: 219 reward: -401.91908212383294\n",
            "episode: 220 reward: -282.5082305011229\n",
            "episode: 221 reward: -135.42191465289923\n",
            "episode: 222 reward: -399.7881535647735\n",
            "episode: 223 reward: -131.06522770318847\n",
            "episode: 224 reward: -130.7681491912167\n",
            "episode: 225 reward: -135.31477016876133\n",
            "episode: 226 reward: -3.914901001828447\n",
            "episode: 227 reward: -134.5129393394648\n",
            "episode: 228 reward: -376.1469783238271\n",
            "episode: 229 reward: -133.09045533066046\n",
            "episode: 230 reward: -383.2750315233141\n",
            "episode: 231 reward: -263.71240275232276\n",
            "episode: 232 reward: -500.0083919266878\n",
            "episode: 233 reward: -135.22531187168758\n",
            "episode: 234 reward: -135.17818433537522\n",
            "episode: 235 reward: -395.9834332194123\n",
            "episode: 236 reward: -126.08778928679216\n",
            "episode: 237 reward: -413.7495701300203\n",
            "episode: 238 reward: -131.37116502717876\n",
            "episode: 239 reward: -121.6506938627967\n",
            "episode: 240 reward: -653.7053929625495\n",
            "episode: 241 reward: -254.87183145095838\n",
            "episode: 242 reward: -129.71331746419523\n",
            "episode: 243 reward: -265.9795936355916\n",
            "episode: 244 reward: -400.65989274385277\n",
            "episode: 245 reward: -251.82522565834446\n",
            "episode: 246 reward: -3.95924871368981\n",
            "episode: 247 reward: -312.7505665224348\n",
            "episode: 248 reward: -135.5875093701436\n",
            "episode: 249 reward: -441.6053043293015\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}